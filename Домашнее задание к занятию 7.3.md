# Домашнее задание к занятию 7.2.Практическое задание по теме «Основы Terraform»

## Задача 1. Создадим бэкэнд в S3 (необязательно, но крайне желательно).

Если в рамках предыдущего задания у вас уже есть аккаунт AWS, то давайте продолжим знакомство со взаимодействием терраформа и aws.

    Создайте s3 бакет, iam роль и пользователя от которого будет работать терраформ. Можно создать отдельного пользователя, а можно использовать созданного в рамках предыдущего задания, просто добавьте ему необходимы права, как описано здесь.
    Зарегистрируйте бэкэнд в терраформ проекте как описано по ссылке выше.

Сделано, в конфиг можно добавить 

     backend "s3" {
        endpoint   = "storage.yandexcloud.net"
        bucket     = "bucketdz7-3"
        region     = "ru-central1"
        key        = "mystatedz/terraform.tfstate"
        access_key = "	"
        secret_key = "	"
    
        skip_region_validation      = true
        skip_credentials_validation = true
      }



## Задача 2. Инициализируем проект и создаем воркспейсы.

    Выполните terraform init:
        если был создан бэкэнд в S3, то терраформ создат файл стейтов в S3 и запись в таблице dynamodb.
        иначе будет создан локальный файл со стейтами.
    Создайте два воркспейса stage и prod.
    В уже созданный aws_instance добавьте зависимость типа инстанса от вокспейса, что бы в разных ворскспейсах использовались разные instance_type.
    Добавим count. Для stage должен создаться один экземпляр ec2, а для prod два.
    Создайте рядом еще один aws_instance, но теперь определите их количество при помощи for_each, а не count.
    Что бы при изменении типа инстанса не возникло ситуации, когда не будет ни одного инстанса добавьте параметр жизненного цикла create_before_destroy = true в один из рессурсов aws_instance.
    При желании поэкспериментируйте с другими параметрами и рессурсами.

В виде результата работы пришлите:

    Вывод команды terraform workspace list.
    Вывод команды terraform plan для воркспейса prod.
Выполялось на Яндекс облаке

    vagrant@vagrant:/vagrant/dz7_3$ terraform workspace list
    default
    * prod
    stage

<details>
    <summary>terraform plan для воркспейса prod</summary>	
	
	vagrant@vagrant:/vagrant/dz7_3$ terraform plan
		
		Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the
		following symbols:
		+ create

	Terraform will perform the following actions:

	  # yandex_compute_image.ubuntu will be created
	  + resource "yandex_compute_image" "ubuntu" {
		  + created_at      = (known after apply)
		  + folder_id       = (known after apply)
		  + id              = (known after apply)
		  + min_disk_size   = (known after apply)
		  + name            = "ubuntu-2004-lts"
		  + os_type         = (known after apply)
		  + pooled          = (known after apply)
		  + product_ids     = (known after apply)
		  + size            = (known after apply)
		  + source_disk     = (known after apply)
		  + source_family   = (known after apply)
		  + source_image    = "fd8qs44945ddtla09hnr"
		  + source_snapshot = (known after apply)
		  + source_url      = (known after apply)
		  + status          = (known after apply)
		}

	  # yandex_compute_instance.vm-count[0] will be created
	  + resource "yandex_compute_instance" "vm-count" {
		  + created_at                = (known after apply)
		  + folder_id                 = (known after apply)
		  + fqdn                      = (known after apply)
		  + hostname                  = (known after apply)
		  + id                        = (known after apply)
		  + metadata                  = {
			  + "ssh-keys" = <<-EOT
					ubuntu:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDTEBt7HLeF1XUUEOqyzdCH1QAsy/apmtT95Jwvno4/APiFzpijNr0fwV3/4Eze70GILS7qP6NxKaNm8j3+0AHpcNMhZyH5zoa9Iwmyqgd+EUA6qMy7tUvVpUoOkMXsYY5N1wYbecaiIfqv+J6sI5LYvFeK14U4q7SQGkoTAQ8H1AoEOiZlFhiH/9C9czncZ2YNq+AFuiw9X/DBJDYwkEfrrcPNMFv7+keAy0Gi4oW+f4+4k21JleYuWd581QHIofCmQyDeju5+CCgG0YVAJsNKFvZSVCm0n+kMM3B+PZJN49iiPIOoS9vC98anSJm5EMMG+yVHq75+Nx1Roj9DENm5vpRVtOmAQSZb45GKFMiy/y/QLZnR1hPlEHDD9BeJF0gyVYMy/XSZd3eywSP3qthSaKS5HfLK43jtWdLLvIHVUKMRN0mI/151u+P55Rg3OctoAK2eBeelfCnkO3ReTdW9n4r1QvSKu5IZUpEiYuERTAy/6fA6KPetnioXbRpcKjc= vagrant@vagrant
				EOT
			}
		  + name                      = "vm-0-prod"
		  + network_acceleration_type = "standard"
		  + platform_id               = "standard-v1"
		  + service_account_id        = (known after apply)
		  + status                    = (known after apply)
		  + zone                      = (known after apply)

		  + boot_disk {
			  + auto_delete = true
			  + device_name = (known after apply)
			  + disk_id     = (known after apply)
			  + mode        = (known after apply)

			  + initialize_params {
				  + block_size  = (known after apply)
				  + description = (known after apply)
				  + image_id    = (known after apply)
				  + name        = (known after apply)
				  + size        = 20
				  + snapshot_id = (known after apply)
				  + type        = "network-hdd"
				}
			}

		  + network_interface {
			  + index              = (known after apply)
			  + ip_address         = (known after apply)
			  + ipv4               = true
			  + ipv6               = false
			  + ipv6_address       = (known after apply)
			  + mac_address        = (known after apply)
			  + nat                = true
			  + nat_ip_address     = (known after apply)
			  + nat_ip_version     = (known after apply)
			  + security_group_ids = (known after apply)
			  + subnet_id          = (known after apply)
			}

		  + placement_policy {
			  + host_affinity_rules = (known after apply)
			  + placement_group_id  = (known after apply)
			}

		  + resources {
			  + core_fraction = 100
			  + cores         = 2
			  + memory        = 2
			}

		  + scheduling_policy {
			  + preemptible = (known after apply)
			}
		}

	  # yandex_compute_instance.vm-count[1] will be created
	  + resource "yandex_compute_instance" "vm-count" {
		  + created_at                = (known after apply)
		  + folder_id                 = (known after apply)
		  + fqdn                      = (known after apply)
		  + hostname                  = (known after apply)
		  + id                        = (known after apply)
		  + metadata                  = {
			  + "ssh-keys" = <<-EOT
					ubuntu:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDTEBt7HLeF1XUUEOqyzdCH1QAsy/apmtT95Jwvno4/APiFzpijNr0fwV3/4Eze70GILS7qP6NxKaNm8j3+0AHpcNMhZyH5zoa9Iwmyqgd+EUA6qMy7tUvVpUoOkMXsYY5N1wYbecaiIfqv+J6sI5LYvFeK14U4q7SQGkoTAQ8H1AoEOiZlFhiH/9C9czncZ2YNq+AFuiw9X/DBJDYwkEfrrcPNMFv7+keAy0Gi4oW+f4+4k21JleYuWd581QHIofCmQyDeju5+CCgG0YVAJsNKFvZSVCm0n+kMM3B+PZJN49iiPIOoS9vC98anSJm5EMMG+yVHq75+Nx1Roj9DENm5vpRVtOmAQSZb45GKFMiy/y/QLZnR1hPlEHDD9BeJF0gyVYMy/XSZd3eywSP3qthSaKS5HfLK43jtWdLLvIHVUKMRN0mI/151u+P55Rg3OctoAK2eBeelfCnkO3ReTdW9n4r1QvSKu5IZUpEiYuERTAy/6fA6KPetnioXbRpcKjc= vagrant@vagrant
				EOT
			}
		  + name                      = "vm-1-prod"
		  + network_acceleration_type = "standard"
		  + platform_id               = "standard-v1"
		  + service_account_id        = (known after apply)
		  + status                    = (known after apply)
		  + zone                      = (known after apply)

		  + boot_disk {
			  + auto_delete = true
			  + device_name = (known after apply)
			  + disk_id     = (known after apply)
			  + mode        = (known after apply)

			  + initialize_params {
				  + block_size  = (known after apply)
				  + description = (known after apply)
				  + image_id    = (known after apply)
				  + name        = (known after apply)
				  + size        = 20
				  + snapshot_id = (known after apply)
				  + type        = "network-hdd"
				}
			}

		  + network_interface {
			  + index              = (known after apply)
			  + ip_address         = (known after apply)
			  + ipv4               = true
			  + ipv6               = false
			  + ipv6_address       = (known after apply)
			  + mac_address        = (known after apply)
			  + nat                = true
			  + nat_ip_address     = (known after apply)
			  + nat_ip_version     = (known after apply)
			  + security_group_ids = (known after apply)
			  + subnet_id          = (known after apply)
			}

		  + placement_policy {
			  + host_affinity_rules = (known after apply)
			  + placement_group_id  = (known after apply)
			}

		  + resources {
			  + core_fraction = 100
			  + cores         = 2
			  + memory        = 2
			}

		  + scheduling_policy {
			  + preemptible = (known after apply)
			}
		}

	  # yandex_compute_instance.vm-for["1"] will be created
	  + resource "yandex_compute_instance" "vm-for" {
		  + created_at                = (known after apply)
		  + folder_id                 = (known after apply)
		  + fqdn                      = (known after apply)
		  + hostname                  = (known after apply)
		  + id                        = (known after apply)
		  + name                      = "vm-1-prod-dz"
		  + network_acceleration_type = "standard"
		  + platform_id               = "standard-v1"
		  + service_account_id        = (known after apply)
		  + status                    = (known after apply)
		  + zone                      = (known after apply)

		  + boot_disk {
			  + auto_delete = true
			  + device_name = (known after apply)
			  + disk_id     = (known after apply)
			  + mode        = (known after apply)

			  + initialize_params {
				  + block_size  = (known after apply)
				  + description = (known after apply)
				  + image_id    = (known after apply)
				  + name        = (known after apply)
				  + size        = (known after apply)
				  + snapshot_id = (known after apply)
				  + type        = "network-hdd"
				}
			}

		  + network_interface {
			  + index              = (known after apply)
			  + ip_address         = (known after apply)
			  + ipv4               = true
			  + ipv6               = false
			  + ipv6_address       = (known after apply)
			  + mac_address        = (known after apply)
			  + nat                = true
			  + nat_ip_address     = (known after apply)
			  + nat_ip_version     = (known after apply)
			  + security_group_ids = (known after apply)
			  + subnet_id          = (known after apply)
			}

		  + placement_policy {
			  + host_affinity_rules = (known after apply)
			  + placement_group_id  = (known after apply)
			}

		  + resources {
			  + core_fraction = 100
			  + cores         = 2
			  + memory        = 4
			}

		  + scheduling_policy {
			  + preemptible = (known after apply)
			}
		}

	  # yandex_compute_instance.vm-for["2"] will be created
	  + resource "yandex_compute_instance" "vm-for" {
		  + created_at                = (known after apply)
		  + folder_id                 = (known after apply)
		  + fqdn                      = (known after apply)
		  + hostname                  = (known after apply)
		  + id                        = (known after apply)
		  + name                      = "vm-2-prod-dz"
		  + network_acceleration_type = "standard"
		  + platform_id               = "standard-v1"
		  + service_account_id        = (known after apply)
		  + status                    = (known after apply)
		  + zone                      = (known after apply)

		  + boot_disk {
			  + auto_delete = true
			  + device_name = (known after apply)
			  + disk_id     = (known after apply)
			  + mode        = (known after apply)

			  + initialize_params {
				  + block_size  = (known after apply)
				  + description = (known after apply)
				  + image_id    = (known after apply)
				  + name        = (known after apply)
				  + size        = (known after apply)
				  + snapshot_id = (known after apply)
				  + type        = "network-hdd"
				}
			}

		  + network_interface {
			  + index              = (known after apply)
			  + ip_address         = (known after apply)
			  + ipv4               = true
			  + ipv6               = false
			  + ipv6_address       = (known after apply)
			  + mac_address        = (known after apply)
			  + nat                = true
			  + nat_ip_address     = (known after apply)
			  + nat_ip_version     = (known after apply)
			  + security_group_ids = (known after apply)
			  + subnet_id          = (known after apply)
			}

		  + placement_policy {
			  + host_affinity_rules = (known after apply)
			  + placement_group_id  = (known after apply)
			}

		  + resources {
			  + core_fraction = 100
			  + cores         = 2
			  + memory        = 4
			}

		  + scheduling_policy {
			  + preemptible = (known after apply)
			}
		}

	  # yandex_vpc_network.net will be created
	  + resource "yandex_vpc_network" "net" {
		  + created_at                = (known after apply)
		  + default_security_group_id = (known after apply)
		  + folder_id                 = (known after apply)
		  + id                        = (known after apply)
		  + labels                    = (known after apply)
		  + name                      = "networkdz72"
		  + subnet_ids                = (known after apply)
		}

	  # yandex_vpc_subnet.subnet will be created
	  + resource "yandex_vpc_subnet" "subnet" {
		  + created_at     = (known after apply)
		  + folder_id      = (known after apply)
		  + id             = (known after apply)
		  + labels         = (known after apply)
		  + name           = "subnet"
		  + network_id     = (known after apply)
		  + v4_cidr_blocks = [
			  + "10.2.0.0/16",
			]
		  + v6_cidr_blocks = (known after apply)
		  + zone           = "ru-central1-a"
		}

	Plan: 7 to add, 0 to change, 0 to destroy.

	───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

	Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if
	you run "terraform apply" now.

</details>